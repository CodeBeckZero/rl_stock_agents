{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update check\n",
    "# Mike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=math.sqrt(25)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for file_name, stock_name in zip(stock_list,stock_name_list):\\n\\n    # Import File\\n    \\n    df_ohlcv = cleandata.NASDAQ_csv_input(file_name,import_path)\\n\\n\\n    env = ContinuousOHLCVEnv(df_ohlcv[[\"open\",\"high\",\"low\",\\'close\\',\"volume\"]].iloc[:2000].to_numpy())\\n\\n\\n    state_size = env.observation_space.shape[0]\\n    action_size = env.action_space.n\\n    available_actions = env.available_actions\\n\\n    # Create a DQN agent\\n    agent = DQNAgent(state_size, action_size,available_actions)\\n\\n    batch_size = 32\\n    EPISODES = 50  # Number of episodes for training\\n\\n    for e in range(EPISODES):\\n        logging.info(e)\\n        env.reset()\\n        state = env.get_observation()\\n        state = np.reshape(state, [1, state_size])\\n        print(available_actions)\\n        for time in range(500):  # Adjust the number of time steps per episode\\n            action, action_type = agent.act(state)\\n            next_state, reward, new_available_actions, done = env.step(action)\\n            logging.info(action_type)\\n            logging.info(env.step_info[-1])\\n            agent.available_actions = new_available_actions\\n            reward = reward if not done else -10  # Modify the reward as needed\\n            next_state = np.reshape(next_state, [1, state_size])\\n            agent.remember(state, action, reward, next_state, done)\\n            state = next_state\\n            if done:\\n                print(f\"episode: {e}/{EPISODES}, score: {time}, epsilon: {agent.epsilon:.2}\")\\n                break\\n            if len(agent.memory) > batch_size:\\n                agent.replay(batch_size)'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "#DQN Agent\n",
    "import sys\n",
    "pwd = \"P:/dsv/DAMII/Practical Project/rl_stock_agents\"\n",
    "sys.path.append(pwd)\n",
    "\n",
    "import cleandata\n",
    "import numpy as np\n",
    "from stockenv import ContinuousOHLCVEnv\n",
    "import matplotlib.pyplot as plt\n",
    "from agentperform import agent_stock_performance\n",
    "import logging\n",
    "\n",
    "\n",
    "\n",
    "stock_list =[\"amzn_daily.csv\"]#, \"appl_daily.csv\",\"tsla_daily.csv\",\"f_daily.csv\",\"pfe_daily.csv\",\n",
    "             #\"coke_daily.csv\",\"brk_daily.csv\",\"nee_daily.csv\",\"jnj_daily.csv\",\"pg_daily.csv\"]\n",
    "\n",
    "stock_name_list = ['AMZN']#,'APPL','TSLA','F','PFE','COKE','BRK','NEE','JNJ','PG']\n",
    "\n",
    "import_path = \"P:/dsv/DAMII/Practical Project/rl_stock_agents/input_data\"\n",
    "\n",
    "\n",
    "# Instantiate the environment\n",
    "df_ohlcv = cleandata.NASDAQ_csv_input(\"amzn_daily.csv\",import_path)\n",
    "env = ContinuousOHLCVEnv(df_ohlcv[[\"open\",\"high\",\"low\",'close',\"volume\"]].iloc[:2000].to_numpy(),commission_rate=0.005)\n",
    "results =[]\n",
    "\n",
    "\"\"\"for file_name, stock_name in zip(stock_list,stock_name_list):\n",
    "\n",
    "    # Import File\n",
    "    \n",
    "    df_ohlcv = cleandata.NASDAQ_csv_input(file_name,import_path)\n",
    "\n",
    "\n",
    "    env = ContinuousOHLCVEnv(df_ohlcv[[\"open\",\"high\",\"low\",'close',\"volume\"]].iloc[:2000].to_numpy())\n",
    "\n",
    "\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "    available_actions = env.available_actions\n",
    "\n",
    "    # Create a DQN agent\n",
    "    agent = DQNAgent(state_size, action_size,available_actions)\n",
    "\n",
    "    batch_size = 32\n",
    "    EPISODES = 50  # Number of episodes for training\n",
    "\n",
    "    for e in range(EPISODES):\n",
    "        logging.info(e)\n",
    "        env.reset()\n",
    "        state = env.get_observation()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "        print(available_actions)\n",
    "        for time in range(500):  # Adjust the number of time steps per episode\n",
    "            action, action_type = agent.act(state)\n",
    "            next_state, reward, new_available_actions, done = env.step(action)\n",
    "            logging.info(action_type)\n",
    "            logging.info(env.step_info[-1])\n",
    "            agent.available_actions = new_available_actions\n",
    "            reward = reward if not done else -10  # Modify the reward as needed\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            if done:\n",
    "                print(f\"episode: {e}/{EPISODES}, score: {time}, epsilon: {agent.epsilon:.2}\")\n",
    "                break\n",
    "            if len(agent.memory) > batch_size:\n",
    "                agent.replay(batch_size)\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Step': 39,\n",
       " 'Portfolio Value': 972.11,\n",
       " 'Cash': 972.0,\n",
       " 'Stock Value': 0.0,\n",
       " 'Stock Holdings': 0.0,\n",
       " 'Stock Price': 17.99,\n",
       " 'Last Reward': 0.0,\n",
       " 'Total Reward': -27.89,\n",
       " 'Last Commission Cost': 4.78,\n",
       " 'Total Commision Cost': 29.54,\n",
       " 'Input': array([1.799300e+01, 1.823750e+01, 1.785850e+01, 1.799000e+01,\n",
       "        7.243306e+07]),\n",
       " 'Available Actions': ('H', 'B'),\n",
       " 'Action': 'H'}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step('H')\n",
    "env.step_info[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
