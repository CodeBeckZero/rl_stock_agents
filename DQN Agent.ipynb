{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\beckm\\Anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\beckm\\Anaconda3\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "(0, 1)\n",
      "1/1 [==============================] - 0s 126ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 55\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(available_actions)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m time \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m500\u001b[39m):  \u001b[38;5;66;03m# Adjust the number of time steps per episode\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m     action, action_type \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m     next_state, reward, new_available_actions, done \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     57\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(action_type)\n",
      "File \u001b[1;32mp:\\DSV\\DAMII\\Practical Project\\rl_stock_agents\\agents.py:40\u001b[0m, in \u001b[0;36mDQNAgent.act\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     38\u001b[0m possible_act_value_idx \u001b[38;5;241m=\u001b[39m [act \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m act \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavailable_actions]\n\u001b[0;32m     39\u001b[0m max_value \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(act_values[possible_act_value_idx])\n\u001b[1;32m---> 40\u001b[0m max_indices \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i, q_value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(act_values) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mabs\u001b[39m(q_value \u001b[38;5;241m-\u001b[39m max_value) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1e-8\u001b[39m]\n\u001b[0;32m     41\u001b[0m valid_max_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(possible_act_value_idx)\u001b[38;5;241m.\u001b[39mintersection(\u001b[38;5;28mset\u001b[39m(max_indices)))\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(act_values,valid_max_indices)\n",
      "File \u001b[1;32mp:\\DSV\\DAMII\\Practical Project\\rl_stock_agents\\agents.py:40\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     38\u001b[0m possible_act_value_idx \u001b[38;5;241m=\u001b[39m [act \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m act \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavailable_actions]\n\u001b[0;32m     39\u001b[0m max_value \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(act_values[possible_act_value_idx])\n\u001b[1;32m---> 40\u001b[0m max_indices \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i, q_value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(act_values) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mabs\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mq_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_value\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-8\u001b[39;49m]\n\u001b[0;32m     41\u001b[0m valid_max_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(possible_act_value_idx)\u001b[38;5;241m.\u001b[39mintersection(\u001b[38;5;28mset\u001b[39m(max_indices)))\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(act_values,valid_max_indices)\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "#DQN Agent\n",
    "import sys\n",
    "pwd = \"P:/dsv/DAMII/Practical Project/rl_stock_agents\"\n",
    "sys.path.append(pwd)\n",
    "\n",
    "import cleandata\n",
    "import numpy as np\n",
    "from stockenv import ContinuousOHLCVEnv\n",
    "import matplotlib.pyplot as plt\n",
    "from agentperform import agent_stock_performance\n",
    "from agents import DQNAgent\n",
    "import logging\n",
    "\n",
    "\n",
    "\n",
    "stock_list =[\"amzn_daily.csv\"]#, \"appl_daily.csv\",\"tsla_daily.csv\",\"f_daily.csv\",\"pfe_daily.csv\",\n",
    "             #\"coke_daily.csv\",\"brk_daily.csv\",\"nee_daily.csv\",\"jnj_daily.csv\",\"pg_daily.csv\"]\n",
    "\n",
    "stock_name_list = ['AMZN']#,'APPL','TSLA','F','PFE','COKE','BRK','NEE','JNJ','PG']\n",
    "\n",
    "import_path = \"P:/dsv/DAMII/Practical Project/rl_stock_agents/input_data\"\n",
    "\n",
    "\n",
    "# Instantiate the environment\n",
    "\n",
    "results =[]\n",
    "\n",
    "for file_name, stock_name in zip(stock_list,stock_name_list):\n",
    "    # Import File\n",
    "    \n",
    "    df_ohlcv = cleandata.NASDAQ_csv_input(file_name,import_path)\n",
    "\n",
    "\n",
    "    env = ContinuousOHLCVEnv(df_ohlcv[[\"open\",\"high\",\"low\",'close',\"volume\"]].iloc[:2000].to_numpy())\n",
    "\n",
    "\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "    available_actions = env.available_actions\n",
    "\n",
    "    # Create a DQN agent\n",
    "    agent = DQNAgent(state_size, action_size,available_actions)\n",
    "\n",
    "    batch_size = 32\n",
    "    EPISODES = 50  # Number of episodes for training\n",
    "\n",
    "    for e in range(EPISODES):\n",
    "        logging.info(e)\n",
    "        env.reset()\n",
    "        state = env.get_observation()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "        print(available_actions)\n",
    "        for time in range(500):  # Adjust the number of time steps per episode\n",
    "            action, action_type = agent.act(state)\n",
    "            next_state, reward, new_available_actions, done = env.step(action)\n",
    "            logging.info(action_type)\n",
    "            logging.info(env.step_info[-1])\n",
    "            agent.available_actions = new_available_actions\n",
    "            reward = reward if not done else -10  # Modify the reward as needed\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            if done:\n",
    "                print(f\"episode: {e}/{EPISODES}, score: {time}, epsilon: {agent.epsilon:.2}\")\n",
    "                break\n",
    "            if len(agent.memory) > batch_size:\n",
    "                agent.replay(batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
